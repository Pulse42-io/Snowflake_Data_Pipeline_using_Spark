{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snowflake - ETL pipeline using Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scope of this project:\n",
    "This exercise is to illustrate a simple Data pipeline using Apache Spark to Extract, Transform & Load data from/to Snowflake database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-requisities\n",
    "* Apache Spark installation (This demo is done using Spark 2.4, Scala 2.11.12)\n",
    "* Snowflake Account (This demo is done using Snowflake cluster of xsmall size)\n",
    "* Cloud storage (This demo is done using AWS S3 storage)\n",
    "* Java jdk version 8 (Scala is still having some issues with Java version > 8. So recommended to use jdk8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import configparser\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import AWS/Snowflake Credentials from Config File\n",
    "* Create a .cfg file in your directory and store the credentials there.\n",
    "* Template of credentials.cfg file is as follows\n",
    "\n",
    "<pre style=\"color:blue\">\n",
    "    \n",
    "[AWS_CREDENTIALS]\n",
    "AWS_ACCESS_KEY_ID=\n",
    "AWS_SECRET_ACCESS_KEY=\n",
    "\n",
    "[AWS_S3]\n",
    "S3_BUCKET=\n",
    "    \n",
    "[SNOWFLAKE]\n",
    "URL=\n",
    "ACCOUNT=\n",
    "USER=\n",
    "PASSWORD=\n",
    "</pre>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('credentials.cfg')\n",
    "SF_URL = config['SNOWFLAKE']['URL']\n",
    "SF_ACCOUNT = config['SNOWFLAKE']['ACCOUNT']\n",
    "SF_USER = config['SNOWFLAKE']['USER']\n",
    "SF_PASSWORD = config['SNOWFLAKE']['PASSWORD']\n",
    "AWS_ACCESS_KEY_ID = config['AWS_CREDENTIALS']['AWS_ACCESS_KEY_ID']\n",
    "AWS_SECRET_ACCESS_KEY = config['AWS_CREDENTIALS']['AWS_SECRET_ACCESS_KEY']\n",
    "S3_BUCKET = config['AWS_S3']['S3_BUCKET']\n",
    "#os.environ['AWS_ACCESS_KEY_ID']=AWS_ACCESS_KEY_ID\n",
    "#os.environ['AWS_SECRET_ACCESS_KEY']=AWS_SECRET_ACCESS_KEY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiate Spark Session\n",
    "\n",
    "* Following packages/jars are needed to enable Snowflake connector and AWS connections. \n",
    "* They will be pulled from Maven Repository during Spark session initiation. https://repo1.maven.org/maven2/\n",
    "* Please note the below Packages are for Spark version 2.4 and Scala version 2.11.12. If you are using different versions of Spark/Scala, then use the corresponding compatible JARs accordingly.\n",
    "\n",
    "Package | Description\n",
    "--------|--------------\n",
    "org.apache.hadoop:hadoop-aws:2.7.3 | Hadoop jars for AWS S3 connection\n",
    "net.snowflake:snowflake-jdbc:3.9.2 | Snowflake JDBC connector\n",
    "net.snowflake:spark-snowflake_2.11:2.5.3-spark_2.4 | Snowflake connector for Spark 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Rakesh_Test_Snowflake\") \\\n",
    "    .config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:2.7.3,net.snowflake:snowflake-jdbc:3.9.2,net.snowflake:spark-snowflake_2.11:2.5.3-spark_2.4\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that the USER should have CREATE STAGE privileges on the schema/tables which are being queried. Because Snowflake connector stages the data during querying. Otherwise Spark throws error saying \"Insufficient authorization privileges\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set AWS credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", AWS_ACCESS_KEY_ID)\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", AWS_SECRET_ACCESS_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Snowflake connection parameters\n",
    "* URL, Account, Username, Password are stored in config file since they should not be exposed in code. Make sure not to push the config file in Public git repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfOptions = {\n",
    "  \"sfURL\" : SF_URL,\n",
    "  \"sfAccount\" : SF_ACCOUNT,\n",
    "  \"sfUser\" : SF_USER,\n",
    "  \"sfPassword\" : SF_PASSWORD,\n",
    "  \"sfDatabase\" : \"DEMO_DB\",\n",
    "  \"sfSchema\" : \"PUBLIC\",\n",
    "  \"sfWarehouse\" : \"DEMO_WH\",\n",
    "  \"sfRole\" : \"SYSADMIN\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <mark>ETL - EXTRACT STEP</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql=\"\"\"select s.S_SUPPKEY, s.S_NAME, n.N_NATIONKEY, n.N_NAME  \n",
    "from supplier s \n",
    "left join nation n \n",
    "on s.s_nationkey = n.n_nationkey;\n",
    "\"\"\"\n",
    " \n",
    "SNOWFLAKE_SOURCE_NAME = \"net.snowflake.spark.snowflake\"\n",
    "df = spark.read.format(SNOWFLAKE_SOURCE_NAME) \\\n",
    "    .options(**sfOptions) \\\n",
    "    .option('query', sql) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S_SUPPKEY</th>\n",
       "      <th>S_NAME</th>\n",
       "      <th>N_NATIONKEY</th>\n",
       "      <th>N_NAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>343300</td>\n",
       "      <td>Supplier#000343300</td>\n",
       "      <td>6</td>\n",
       "      <td>FRANCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>343301</td>\n",
       "      <td>Supplier#000343301</td>\n",
       "      <td>14</td>\n",
       "      <td>KENYA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  S_SUPPKEY              S_NAME N_NATIONKEY  N_NAME\n",
       "0    343300  Supplier#000343300           6  FRANCE\n",
       "1    343301  Supplier#000343301          14   KENYA"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <mark>ETL - TRANSFORM STEP</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Spark Transformation\n",
    "  * Group data by Nation Name and get the counts for each Nation\n",
    "  * Output data will have 2 columns - Nation Name, Supplier Nation Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stage1 = spark.sql(\"select n_name, count(*) as supplier_count from df_view group by 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Transformed data into S3 bucket (To be consumed by Snowflake in next step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_s3_path = f'{S3_BUCKET}/supplier_nation_counts'\n",
    "df_stage1.write.mode('overwrite').csv(target_s3_path, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Validation\n",
    "* Validate whether the data is loaded into S3 properly by reading the loaded S3 data and get the count. Count should not be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.csv(target_s3_path, header=True).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <mark>ETL - LOAD STEP</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy data from S3 output bucket into Snowflake Target table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method 1**. Write Spark Dataframe directly into Snowflake Target Table (Target Table name :  supplier_nation_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNOWFLAKE_SOURCE_NAME = \"net.snowflake.spark.snowflake\"\n",
    "df_stage1.write.format(SNOWFLAKE_SOURCE_NAME) \\\n",
    "    .options(**sfOptions) \\\n",
    "    .option(\"dbtable\", \"supplier_nation_counts\") \\\n",
    "    .mode('overwrite') \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method 2**. COPY command (Bulk Copy) - Can be run using script or in Snowflake directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```copy into supplier_nation_counts\n",
    "from target_s3_path credentials=(aws_key_id=AWS_ACCESS_KEY_ID aws_secret_key='AWS_SECRET_ACCESS_KEY')\n",
    "file_format = (type = csv field_delimiter = ',' skip_header = 1);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally!!! -- Read data from Snowflake Target table to verify whether the data is loaded properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+\n",
      "|N_NAME|SUPPLIER_COUNT|\n",
      "+------+--------------+\n",
      "|FRANCE|         40126|\n",
      "| JAPAN|         39862|\n",
      "+------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql=\"select * from supplier_nation_counts\"\n",
    " \n",
    "SNOWFLAKE_SOURCE_NAME = \"net.snowflake.spark.snowflake\"\n",
    "df2 = spark.read.format(SNOWFLAKE_SOURCE_NAME) \\\n",
    "    .options(**sfOptions) \\\n",
    "    .option('query', sql) \\\n",
    "    .load()\n",
    "df2.show(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
